{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from plotutils import plot_data, plot_surface\n",
    "from SVM import kernel_svm_train, kernel_svm_predict\n",
    "from sklearn import preprocessing\n",
    "from crossval import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38000,)\n",
      "(38000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Read the data using pandas\n",
    "df = pd.read_csv('digit-recognition/digit-recognition/train.csv')\n",
    "df2 = pd.read_csv('digit-recognition/digit-recognition/test.csv')\n",
    "\n",
    "#turn the training dataframe into numpy array \n",
    "label_data_train = df.to_numpy()\n",
    "\n",
    "#column vector for the labels (38000,)\n",
    "label_train = label_data_train[:,0] \n",
    "\n",
    "# (38000 x 784 matrix for the training data )\n",
    "data_train = label_data_train[:, 1:] \n",
    "print(label_train.shape)\n",
    "print(data_train.shape)\n",
    "\n",
    "# Turn testing dataframe into numpy array \n",
    "label_data_test = df2.to_numpy()\n",
    "label_test = label_data_test[:,0]\n",
    "data_test = label_data_test[:,1:]\n",
    "\n",
    "masked_train_labels = np.full(label_train.shape, -1)\n",
    "masked_test_labels = np.full(label_test.shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vals = 10 ** np.linspace(-3, 1, 5)\n",
    "sigmas = np.linspace(0.1, 1.5, 15)\n",
    "orders = [2, 3, 4, 5]\n",
    "norms = ['l1', 'l2', 'max']\n",
    "num_folds = 4\n",
    "sets = len(np.unique(label_train))\n",
    "test_accuracy = np.zeros(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train1 = np.where(label_train==1, label_train, -1)\n",
    "label_train2 = np.where(label_train==2, 1, -1)\n",
    "label_train3 = np.where(label_train==3, 1, -1)\n",
    "label_train4 = np.where(label_train==4, 1, -1)\n",
    "label_train5 = np.where(label_train==5, 1, -1)\n",
    "label_train6 = np.where(label_train==6, 1, -1)\n",
    "label_train7 = np.where(label_train==7, 1, -1)\n",
    "label_train8 = np.where(label_train==8, 1, -1)\n",
    "label_train9 = np.where(label_train==9, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "(8219, 784)\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into OVO \n",
    "#Create all the models \n",
    "#start index will be 1\n",
    "list_of_models = []\n",
    "list_of_labels = []\n",
    "for i in range(1,10): \n",
    "    for j in range(i+1,10):\n",
    "        label_traini_j = np.where(np.logical_or(label_train==i,label_train==j))[0] #either i or j\n",
    "        data_traini_j = data_train[label_traini_j,:]\n",
    "        labels_i_j = label_train[label_traini_j]\n",
    "        labels_i_j = np.where(labels_i_j==i, 1, -1)\n",
    "        list_of_models.append(data_traini_j)\n",
    "        list_of_labels.append(labels_i_j)\n",
    "print(len(list_of_models))\n",
    "print(list_of_models[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 0: 0.452375\n",
      "Test accuracy for 1: 0.452375\n",
      "Test accuracy for 2: 0.452375\n",
      "Test accuracy for 3: 0.452375\n",
      "Test accuracy for 4: 0.452375\n",
      "Test accuracy for 5: 0.452375\n",
      "Test accuracy for 6: 0.452375\n",
      "Test accuracy for 7: 0.452375\n",
      "Test accuracy for 8: 0.452375\n",
      "Test accuracy for 9: 0.452375\n",
      "Test accuracy for 10: 0.452375\n",
      "Test accuracy for 11: 0.452375\n",
      "Test accuracy for 12: 0.452375\n",
      "Test accuracy for 13: 0.452375\n",
      "Test accuracy for 14: 0.452375\n",
      "Test accuracy for 15: 0.452375\n",
      "Test accuracy for 16: 0.452375\n",
      "Test accuracy for 17: 0.452375\n",
      "Test accuracy for 18: 0.452375\n",
      "Test accuracy for 19: 0.452375\n",
      "Test accuracy for 20: 0.452375\n",
      "Test accuracy for 21: 0.452375\n",
      "Test accuracy for 22: 0.452375\n",
      "Test accuracy for 23: 0.452375\n",
      "Test accuracy for 24: 0.452375\n",
      "Test accuracy for 25: 0.452375\n",
      "Test accuracy for 26: 0.452375\n",
      "Test accuracy for 27: 0.452375\n",
      "Test accuracy for 28: 0.452375\n",
      "Test accuracy for 29: 0.452375\n",
      "Test accuracy for 30: 0.452375\n",
      "Test accuracy for 31: 0.452375\n",
      "Test accuracy for 32: 0.452375\n",
      "Test accuracy for 33: 0.452375\n",
      "Test accuracy for 34: 0.452375\n",
      "Test accuracy for 35: 0.452375\n",
      "Overall accuracy: 0.4523750000000001\n"
     ]
    }
   ],
   "source": [
    "# Run rbf kernel with no cross validation (random value selection)\n",
    "predictions = []\n",
    "for i in range(len(list_of_models)):\n",
    "    train = list_of_models[i]\n",
    "    train_labels = list_of_labels[i]\n",
    "    best_params = {}\n",
    "    best_params['kernel'] = 'rbf'\n",
    "    best_params['C'] = 1\n",
    "    best_params['sigma'] = .1\n",
    "   # print(train)\n",
    "    split_train_data = train[1:500,:]\n",
    "    split_train_labels = train_labels[1:500]\n",
    "    \n",
    "    split_train_data_normalized = preprocessing.normalize(split_train_data, norm='l2')\n",
    "\n",
    "    model = kernel_svm_train(split_train_data_normalized.T, split_train_labels, best_params)\n",
    "    predictions.append(kernel_svm_predict(data_test.T, model))\n",
    "    #will need to play around with this a bit \n",
    "   # test_accuracy[i] = np.mean(predictions[0] == masked_test_labels)\n",
    "    print(\"Test accuracy for {0}: {1}\".format(i, test_accuracy[i]))\n",
    "    \n",
    "print(\"Overall accuracy: {0}\".format(np.mean(test_accuracy))) #this is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
